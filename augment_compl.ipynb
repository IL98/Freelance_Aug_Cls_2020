{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VyzZUfFk0nq8"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-efefb19f0efc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OwAuBA7R0pYn"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset_with_targets.csv', header=None)\n",
    "\n",
    "val_size = 5000\n",
    "\n",
    "test_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "jvExerN32oRC",
    "outputId": "c1855e12-c8d9-4356-ae61-7c6aaec0e611"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.077409</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>-90.369800</td>\n",
       "      <td>53.700094</td>\n",
       "      <td>-59.154930</td>\n",
       "      <td>0.276798</td>\n",
       "      <td>0.021783</td>\n",
       "      <td>11.644999</td>\n",
       "      <td>34.539900</td>\n",
       "      <td>-0.149500</td>\n",
       "      <td>2.372547</td>\n",
       "      <td>2.116338</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>-305.75</td>\n",
       "      <td>-0.908867</td>\n",
       "      <td>0.051593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.126186</td>\n",
       "      <td>350.0</td>\n",
       "      <td>-86.317435</td>\n",
       "      <td>77.264642</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.450444</td>\n",
       "      <td>0.121033</td>\n",
       "      <td>6.811875</td>\n",
       "      <td>142.023494</td>\n",
       "      <td>-0.381000</td>\n",
       "      <td>0.772327</td>\n",
       "      <td>-0.003629</td>\n",
       "      <td>350.0</td>\n",
       "      <td>-244.75</td>\n",
       "      <td>0.893989</td>\n",
       "      <td>0.124688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.80</td>\n",
       "      <td>-0.398406</td>\n",
       "      <td>300.0</td>\n",
       "      <td>-86.232217</td>\n",
       "      <td>53.064664</td>\n",
       "      <td>-53.333333</td>\n",
       "      <td>0.014723</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>7.820611</td>\n",
       "      <td>57.045273</td>\n",
       "      <td>0.002820</td>\n",
       "      <td>7.115817</td>\n",
       "      <td>6.407732</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.239919</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.042877</td>\n",
       "      <td>4684.0</td>\n",
       "      <td>1634.814815</td>\n",
       "      <td>53.680000</td>\n",
       "      <td>-38.888889</td>\n",
       "      <td>0.024397</td>\n",
       "      <td>0.001594</td>\n",
       "      <td>10.776373</td>\n",
       "      <td>26.110229</td>\n",
       "      <td>0.052905</td>\n",
       "      <td>6.099106</td>\n",
       "      <td>1.952781</td>\n",
       "      <td>-4684.0</td>\n",
       "      <td>10.71</td>\n",
       "      <td>-0.310448</td>\n",
       "      <td>0.268817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.057443</td>\n",
       "      <td>3020.0</td>\n",
       "      <td>-77.865729</td>\n",
       "      <td>55.213228</td>\n",
       "      <td>-25.757576</td>\n",
       "      <td>0.708303</td>\n",
       "      <td>0.079423</td>\n",
       "      <td>8.518989</td>\n",
       "      <td>69.867770</td>\n",
       "      <td>-0.208755</td>\n",
       "      <td>0.770984</td>\n",
       "      <td>0.061754</td>\n",
       "      <td>-3020.0</td>\n",
       "      <td>-443.74</td>\n",
       "      <td>-0.406361</td>\n",
       "      <td>0.083654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0         1       2            3   ...      13      14        15        16\n",
       "0  0.19  0.077409  1250.0   -90.369800  ...  1250.0 -305.75 -0.908867  0.051593\n",
       "1 -0.26  0.126186   350.0   -86.317435  ...   350.0 -244.75  0.893989  0.124688\n",
       "2 -0.80 -0.398406   300.0   -86.232217  ...  -300.0   -0.00  0.239919  0.400000\n",
       "3  0.54 -0.042877  4684.0  1634.814815  ... -4684.0   10.71 -0.310448  0.268817\n",
       "4  0.11 -0.057443  3020.0   -77.865729  ... -3020.0 -443.74 -0.406361  0.083654\n",
       "\n",
       "[5 rows x 17 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8BXxnVhff_rC"
   },
   "outputs": [],
   "source": [
    "train_data = data.iloc[:-(test_size + val_size)]\n",
    "\n",
    "val_data = data.iloc[-(test_size + val_size):-test_size]\n",
    "\n",
    "test_data = data.iloc[-test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCEl7VsaZ6Q3"
   },
   "outputs": [],
   "source": [
    "#using of standard uniform scale to normalize \n",
    "\n",
    "normalizer = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "X_train = normalizer.fit_transform(train_data.values)\n",
    "\n",
    "X_val = normalizer.transform(val_data.values)\n",
    "\n",
    "X_test = normalizer.transform(test_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "g9ne1f2v0skp",
    "outputId": "6c5c8743-78c2-44d9-ee29-9a5374d12448"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8OGF4WMB0tuu"
   },
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self, intermediate_dims, latent_dim, input_shape):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        intermediate_dims - list of ints (with len equal to the number of dense layers in encoder)\n",
    "                          List of dims of dense layers. The last number is the\n",
    "                          dimension of latent space\n",
    "        latent_dim - int, dimension of mu and std vectors\n",
    "\n",
    "        input_shape - tuple, shape of an input sample \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.register_buffer('_initial_mu', torch.zeros((latent_dim)))\n",
    "        self.register_buffer('_initial_sigma', torch.ones((latent_dim)))\n",
    "\n",
    "        self.latent_distribution = torch.distributions.normal.Normal(\n",
    "            loc=self._initial_mu,\n",
    "            scale=self._initial_sigma\n",
    "        )\n",
    "        input_dim = np.prod(input_shape)\n",
    "        self.encoder = nn.Sequential(*[\n",
    "            nn.Linear(input_dim, intermediate_dims[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(intermediate_dims[0]),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(intermediate_dims[0], intermediate_dims[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(intermediate_dims[1]),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(intermediate_dims[1], intermediate_dims[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(intermediate_dims[2]),\n",
    "            nn.Dropout(0.3)\n",
    "        ])\n",
    "\n",
    "        self.mu_repr = nn.Linear(intermediate_dims[2], latent_dim)\n",
    "        self.log_sigma_repr = nn.Linear(intermediate_dims[2], latent_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(*[\n",
    "            nn.Linear(latent_dim, intermediate_dims[2]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(intermediate_dims[2]),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(intermediate_dims[2], intermediate_dims[1]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(intermediate_dims[1]),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(intermediate_dims[1], intermediate_dims[0]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(intermediate_dims[0]),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(intermediate_dims[0], input_dim),\n",
    "            nn.Sigmoid()\n",
    "        ])\n",
    "    \n",
    "    def _encode(self, x):\n",
    "        latent_repr = self.encoder(x)\n",
    "        mu_values = self.mu_repr(latent_repr)\n",
    "        log_sigma_values = self.log_sigma_repr(latent_repr)\n",
    "        return mu_values, log_sigma_values, latent_repr\n",
    "\n",
    "    def decode(self, latent_sample):\n",
    "      return self.decoder(latent_sample)\n",
    "    \n",
    "    def _reparametrize(self, sample, mu_values, log_sigma_values):\n",
    "        latent_sample = torch.exp(log_sigma_values) * sample + mu_values\n",
    "        return latent_sample\n",
    "\n",
    "    def forward(self, x, raw_sample=None):\n",
    "        mu_values, log_sigma_values, latent_repr = self._encode(x)\n",
    "\n",
    "        if raw_sample is None:\n",
    "            raw_sample = torch.randn_like(mu_values)\n",
    "\n",
    "        latent_sample = self._reparametrize(raw_sample, mu_values, log_sigma_values)\n",
    "        \n",
    "        reconstructed_repr = self.decoder(latent_sample)\n",
    "        \n",
    "        return reconstructed_repr, latent_sample, mu_values, log_sigma_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vUlco53G0vA2"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = VariationalAutoEncoder([16, 12, 8], 8, X_train[0].shape).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ooyVCCYo0xzM"
   },
   "outputs": [],
   "source": [
    "# loss compution - the linear combination of reconstuction_loss and KL divergence\n",
    "\n",
    "def compute_loss(batch_x):\n",
    "    batch_x = torch.FloatTensor(batch_x).to(device)\n",
    "\n",
    "    [predictions, latent, mu_values, log_sigma_values] = model(batch_x.to(device))\n",
    "    \n",
    "    kl_loss = 0.5 * torch.mean(torch.sum(\n",
    "          mu_values.pow(2) + torch.exp(log_sigma_values) - 1. - log_sigma_values,\n",
    "          dim=1\n",
    "      ))\n",
    "    bce_loss = loss_func(predictions, batch_x)\n",
    "    \n",
    "    return kl_loss, bce_loss, (bce_loss + kl_loss)/2./batch_x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3r8_sqes03kg"
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(X, batchsize, shuffle=True):\n",
    "    indices = np.arange(len(X))\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(indices)\n",
    "    for start in range(0, len(indices), batchsize):\n",
    "        ix = indices[start: start + batchsize]\n",
    "        yield X[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PhA3a8pP045P",
    "outputId": "6a0b124f-1f34-4ef9-b389-f91ad4b86d04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 150 took 7.009s\n",
      "Training loss=0.091291, KL divergence=2.3843787, BCE Loss=0.719516\n",
      "Val loss=0.057942, KL divergence=1.2836504, BCE Loss=0.686387\n",
      "Epoch 6 of 150 took 6.908s\n",
      "Training loss=0.021533, KL divergence=0.1337659, BCE Loss=0.598361\n",
      "Val loss=0.017943, KL divergence=0.0201699, BCE Loss=0.589893\n",
      "Epoch 11 of 150 took 6.890s\n",
      "Training loss=0.017626, KL divergence=0.0106856, BCE Loss=0.588609\n",
      "Val loss=0.017236, KL divergence=0.0005467, BCE Loss=0.585462\n",
      "Epoch 16 of 150 took 6.834s\n",
      "Training loss=0.017295, KL divergence=0.0004862, BCE Loss=0.587550\n",
      "Val loss=0.017227, KL divergence=0.0000125, BCE Loss=0.585709\n",
      "Epoch 21 of 150 took 6.920s\n",
      "Training loss=0.017265, KL divergence=0.0000006, BCE Loss=0.587010\n",
      "Val loss=0.017210, KL divergence=0.0000004, BCE Loss=0.585137\n",
      "Epoch 26 of 150 took 6.960s\n",
      "Training loss=0.017257, KL divergence=0.0000004, BCE Loss=0.586741\n",
      "Val loss=0.017231, KL divergence=0.0000002, BCE Loss=0.585860\n",
      "Epoch 31 of 150 took 7.035s\n",
      "Training loss=0.017252, KL divergence=0.0000003, BCE Loss=0.586561\n",
      "Val loss=0.017236, KL divergence=0.0000001, BCE Loss=0.586008\n",
      "Epoch 36 of 150 took 6.973s\n",
      "Training loss=0.017248, KL divergence=0.0000002, BCE Loss=0.586444\n",
      "Val loss=0.017217, KL divergence=0.0000001, BCE Loss=0.585379\n",
      "Epoch 41 of 150 took 7.080s\n",
      "Training loss=0.017246, KL divergence=0.0000002, BCE Loss=0.586365\n",
      "Val loss=0.017224, KL divergence=0.0000001, BCE Loss=0.585615\n",
      "Epoch 46 of 150 took 6.947s\n",
      "Training loss=0.017245, KL divergence=0.0000001, BCE Loss=0.586323\n",
      "Val loss=0.017223, KL divergence=0.0000000, BCE Loss=0.585577\n",
      "Epoch 51 of 150 took 6.986s\n",
      "Training loss=0.017243, KL divergence=0.0000001, BCE Loss=0.586264\n",
      "Val loss=0.017223, KL divergence=0.0000000, BCE Loss=0.585582\n",
      "Epoch 56 of 150 took 6.961s\n",
      "Training loss=0.017241, KL divergence=0.0000001, BCE Loss=0.586209\n",
      "Val loss=0.017218, KL divergence=0.0000000, BCE Loss=0.585417\n",
      "Epoch 61 of 150 took 6.963s\n",
      "Training loss=0.017241, KL divergence=0.0000000, BCE Loss=0.586198\n",
      "Val loss=0.017217, KL divergence=0.0000000, BCE Loss=0.585391\n",
      "Epoch 66 of 150 took 6.981s\n",
      "Training loss=0.017240, KL divergence=0.0000000, BCE Loss=0.586158\n",
      "Val loss=0.017212, KL divergence=0.0000000, BCE Loss=0.585207\n",
      "Epoch 71 of 150 took 6.948s\n",
      "Training loss=0.017239, KL divergence=0.0000000, BCE Loss=0.586133\n",
      "Val loss=0.017226, KL divergence=0.0000000, BCE Loss=0.585694\n",
      "Epoch 76 of 150 took 6.946s\n",
      "Training loss=0.017238, KL divergence=0.0000000, BCE Loss=0.586103\n",
      "Val loss=0.017234, KL divergence=0.0000000, BCE Loss=0.585969\n",
      "Epoch 81 of 150 took 6.963s\n",
      "Training loss=0.017237, KL divergence=0.0000000, BCE Loss=0.586073\n",
      "Val loss=0.017225, KL divergence=0.0000000, BCE Loss=0.585655\n",
      "Epoch 86 of 150 took 7.036s\n",
      "Training loss=0.017238, KL divergence=0.0000000, BCE Loss=0.586077\n",
      "Val loss=0.017212, KL divergence=0.0000000, BCE Loss=0.585207\n",
      "Epoch 91 of 150 took 7.010s\n",
      "Training loss=0.017237, KL divergence=0.0000000, BCE Loss=0.586046\n",
      "Val loss=0.017225, KL divergence=0.0000000, BCE Loss=0.585654\n",
      "Epoch 96 of 150 took 7.005s\n",
      "Training loss=0.017236, KL divergence=0.0000000, BCE Loss=0.586038\n",
      "Val loss=0.017213, KL divergence=0.0000000, BCE Loss=0.585245\n",
      "Epoch 101 of 150 took 6.961s\n",
      "Training loss=0.017236, KL divergence=0.0000000, BCE Loss=0.586021\n",
      "Val loss=0.017233, KL divergence=0.0000000, BCE Loss=0.585908\n",
      "Epoch 106 of 150 took 7.011s\n",
      "Training loss=0.017236, KL divergence=0.0000000, BCE Loss=0.586008\n",
      "Val loss=0.017229, KL divergence=0.0000000, BCE Loss=0.585772\n",
      "Epoch 111 of 150 took 6.963s\n",
      "Training loss=0.017235, KL divergence=0.0000000, BCE Loss=0.585998\n",
      "Val loss=0.017225, KL divergence=0.0000000, BCE Loss=0.585657\n",
      "Epoch 116 of 150 took 6.989s\n",
      "Training loss=0.017235, KL divergence=0.0000000, BCE Loss=0.585983\n",
      "Val loss=0.017220, KL divergence=0.0000000, BCE Loss=0.585479\n",
      "Epoch 121 of 150 took 7.130s\n",
      "Training loss=0.017235, KL divergence=0.0000000, BCE Loss=0.585979\n",
      "Val loss=0.017218, KL divergence=0.0000000, BCE Loss=0.585410\n",
      "Epoch 126 of 150 took 7.058s\n",
      "Training loss=0.017235, KL divergence=0.0000000, BCE Loss=0.585977\n",
      "Val loss=0.017231, KL divergence=0.0000000, BCE Loss=0.585847\n",
      "Epoch 131 of 150 took 7.056s\n",
      "Training loss=0.017234, KL divergence=0.0000000, BCE Loss=0.585971\n",
      "Val loss=0.017219, KL divergence=-0.0000000, BCE Loss=0.585437\n",
      "Epoch 136 of 150 took 6.996s\n",
      "Training loss=0.017234, KL divergence=0.0000000, BCE Loss=0.585964\n",
      "Val loss=0.017214, KL divergence=0.0000000, BCE Loss=0.585266\n",
      "Epoch 141 of 150 took 6.950s\n",
      "Training loss=0.017234, KL divergence=0.0000000, BCE Loss=0.585965\n",
      "Val loss=0.017214, KL divergence=0.0000000, BCE Loss=0.585280\n",
      "Epoch 146 of 150 took 7.008s\n",
      "Training loss=0.017234, KL divergence=0.0000000, BCE Loss=0.585956\n",
      "Val loss=0.017221, KL divergence=0.0000000, BCE Loss=0.585512\n"
     ]
    }
   ],
   "source": [
    "#training process\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "loss_func = torch.nn.modules.loss.BCELoss()  #reconstruction loss\n",
    "\n",
    "epochs = 150\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "model.train()\n",
    "loss_accumulator = []\n",
    "bce_acc = []\n",
    "kl_acc = []\n",
    "\n",
    "val_loss_accumulator = []\n",
    "val_bce_acc = []\n",
    "val_kl_acc = []\n",
    "\n",
    "for epoch_num in range(epochs):\n",
    "  start_time = time.time()\n",
    "\n",
    "  model.train(True)\n",
    "  for batch_x in iterate_minibatches(X_train, batch_size):\n",
    "    kl_loss, bce_loss, loss = compute_loss(batch_x)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    loss_accumulator.append(float(loss))\n",
    "    bce_acc.append(float(bce_loss))\n",
    "    kl_acc.append(float(kl_loss))\n",
    "\n",
    "  model.train(False)\n",
    "\n",
    "  for batch_x in iterate_minibatches(X_val, batch_size):\n",
    "    kl_loss, bce_loss, loss = compute_loss(batch_x)\n",
    "\n",
    "    val_loss_accumulator.append(float(loss))\n",
    "    val_bce_acc.append(float(bce_loss))\n",
    "    val_kl_acc.append(float(kl_loss))\n",
    "\n",
    "  if epoch_num % 5 == 0:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "          epoch_num + 1, epochs, time.time() - start_time))\n",
    "    print(\"Training loss={:.6f}, KL divergence={:.7f}, BCE Loss={:.6f}\".format(\n",
    "        np.mean(loss_accumulator[-len(X_train) // batch_size :]),\n",
    "        np.mean(kl_acc[-len(X_train) // batch_size :]),\n",
    "        np.mean(bce_acc[-len(X_train) // batch_size :])))\n",
    "    \n",
    "    print(\"Val loss={:.6f}, KL divergence={:.7f}, BCE Loss={:.6f}\".format(\n",
    "        np.mean(val_loss_accumulator[-len(X_val) // batch_size :]),\n",
    "        np.mean(val_kl_acc[-len(X_val) // batch_size :]),\n",
    "        np.mean(val_bce_acc[-len(X_val) // batch_size :])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-y2TdT8V7p9o"
   },
   "source": [
    "augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0mnkuAT-fJk"
   },
   "outputs": [],
   "source": [
    "def augmentation_forward(model, data):\n",
    "    \"\"\"\n",
    "    simple augmentation - using original latent_sample as a sample to decode\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    generated data - with shape of data\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    _, _, latent_samples = model._encode(torch.FloatTensor(data).to(device))\n",
    "\n",
    "    return model.decode(latent_samples).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nl6dlUnMRiyh"
   },
   "outputs": [],
   "source": [
    "def augmentation_add_noise(model, data, alpha=0.1):\n",
    "    \"\"\"\n",
    "    generating via adding noise to the latent samples of orignal data\n",
    "    \n",
    "    z_i_new = z_i + alpha*\\ksi , where \\ksi from normal(0, sigma), where\n",
    "                          sigma is a sample standard deviation of latent samples\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha - int, regularization for pertubation\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    generated data - with shape of data\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    _, _, latent_samples = model._encode(torch.FloatTensor(data).to(device))\n",
    "    latent_samples = latent_samples.detach().cpu().numpy()\n",
    "    \n",
    "    sigma = latent_samples.std(axis=0)\n",
    "\n",
    "    new_latent_samples = latent_samples + alpha*np.random.normal(scale=sigma, \n",
    "                                                      size=latent_samples.shape)\n",
    "\n",
    "    return model.decode(torch.FloatTensor(new_latent_samples).to(device)).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGCAvnvw8oNz"
   },
   "outputs": [],
   "source": [
    "def augmentation_extrapolation(model, data, k=3, alpha=0.5):\n",
    "    \"\"\"\n",
    "    generating via extrapolations of the latent samples\n",
    "    using the nearest neighbours\n",
    "\n",
    "    z_i_new = (z_k - z_i)*alpha + z_i, where z_k is the k-th nearest neighbor \n",
    "    in the latent space\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k - int, number of neighbours used in KNN\n",
    "    alpha -\n",
    "    len(data)*k - number of generated data\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    generated data - with shape (data.shape[0]*k, data.shape[1])\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    _, _, latent_samples = model._encode(torch.FloatTensor(data).to(device))\n",
    "    latent_samples = latent_samples.detach().cpu()\n",
    "\n",
    "    neigh = NearestNeighbors(n_neighbors=k+1)\n",
    "    neigh.fit(latent_samples)\n",
    "\n",
    "    ids = neigh.kneighbors(latent_samples, return_distance=False)\n",
    "\n",
    "    #extrapolations between closest latent vectors\n",
    "    extrs = (latent_samples[ids] - latent_samples[:, np.newaxis])[:,1:,:]*alpha + \\\n",
    "                       latent_samples[:, np.newaxis]\n",
    "\n",
    "    extrs =  extrs.reshape(extrs.shape[0]*extrs.shape[1],\n",
    "                           extrs.shape[2])\n",
    "\n",
    "    return model.decode(torch.FloatTensor(extrs).to(device)).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5D1O5Zmn0BG-"
   },
   "outputs": [],
   "source": [
    "generated_data_1 = augmentation_forward(model, np.concatenate((X_train, X_val), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p7_oKMWZ-Wvx"
   },
   "outputs": [],
   "source": [
    "generated_data_2 = augmentation_extrapolation(model, np.concatenate((X_train, X_val), axis=0), k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qCXvZZyJvg1M"
   },
   "outputs": [],
   "source": [
    "generated_data_3 = augmentation_add_noise(model, np.concatenate((X_train, X_val), axis=0), alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "In55nOZ72Lgg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VyqRKM_N2UHC"
   },
   "source": [
    "converting data to original format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hTfPtq8e2LpL"
   },
   "outputs": [],
   "source": [
    "def to_original_format(generated_data, normalizer):\n",
    "  \"\"\"\n",
    "  noramlizer - to denormalize data\n",
    "  \"\"\"\n",
    "  descaled = normalizer.inverse_transform(generated_data)\n",
    "\n",
    "  descaled[:, [2, 13]] = np.round(descaled[:, [2, 13]])\n",
    "\n",
    "  gen_data = pd.DataFrame(descaled)\n",
    "\n",
    "  return gen_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZSAoB5kG5jsp"
   },
   "outputs": [],
   "source": [
    "to_original_format(generated_data_1, normalizer).to_csv(\"gen_data_1.csv\", header=None, index=False)\n",
    "to_original_format(generated_data_2, normalizer).to_csv(\"gen_data_2.csv\", header=None, index=False)\n",
    "to_original_format(generated_data_3, normalizer).to_csv(\"gen_data_3.csv\", header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gp5dQE3-2JS5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5HzMM6Kzkd78"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "augment_compl.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
